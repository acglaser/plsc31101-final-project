---
title: "Final Project Narrative"
author: "Audrey Glaser"
date: "12/9/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Brief substantive background / goal

Google BigQuery is an enterprise cloud data warehouse that is specially suited for storing geospatial data and machine learning training data. It is also where Google hosts a number of public datasets of interest to political scientists, including Google's political advertising data, which is now available to the public.

The goal of this tutorial is to show how to access the Google BigQuery API using the R library bigrquery. It will cover initializing access, creating a query, and a brief analysis of the structure of the 2020 political advertising dataset.

## Initializing access

There are several steps. 

First, you must set up a BigQuery server account key. Unlike Twitter where authentication is completed using your Twitter user account, Google recommends creating a separate service account to establish your BigQuery credentials. 

This is done by navigating to the Google Cloud Console. You'll need to log in with your normal Google account. Once in the dashboard, you go to the Credentials tab, click on the Create Credentials dropdown, and select Service Account Key. From here you will follow the directions. When you complete the form, your browser will automatically download a .JSON file with your private key. It will also appear in your Credentials in the web UI.

Second, you must create a BigQuery project. Typically, this project is where you'd host your own data in the Google Cloud, and the Project ID is how you'd link your query to your data. For the case of accessing public data, you will use the Project ID as a value for the "billing" argument. Google sets rate limits at the account and the project level, so it needs to know which project to "bill" your data usage to. You do this in the console as well, clicking the Project icon in the menu bar.

Third, you enable API access within your project. This is done using the WYSIWYG editor in the Google Cloud Console. 

When all these steps are completed, you are ready to set up access in R.

You must download and load the bigrquery library. Then, run the bq_auth() function and paste in the file name of your .JSON file or the key as it appears in the web UI. 

## How to query

'bigrquery' has three layers of wrapping the UI. 

The low-level API provides a thin wrapper over the underlying REST API. It requires you to write your own SQL queries and be familiar with the BigQuery API's idiosyncracies. These functions take the form bq_noun_verb(). 

The DBI interface wraps the low-level API. You still need to write your own queries but the functions and classes are defined by DBI, which is a generic database interface in R. These functions usually begin with the prefix "db"

Finally, the dplyr interface lets you treat BigQuery tables like in-house data. No SQL required. The actual package used here is dbplyr, which is dplyr but for remote databases.

We will need dbConnect() to get started.

Then you can use commands you're familiar with.

An important aspect of dbplyr is that it's lazy. It will only pull data down from the cloud when you explicitly request it, and even then, it will do the bare minimum.

For example, as you write your code, dbplyr is translating your code into SQL, but it will not submit the query to the API until you print the output. Even then, it will only show a preview. You need the function collect() to tell bigrquery to pull down all the requested data into your loal dataframe.

## Troubleshooting



## Future work

